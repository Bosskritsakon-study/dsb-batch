{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AheHgdoQ4mfi",
    "outputId": "6d159b83-2809-4723-d2c2-0417c9ad77fc"
   },
   "outputs": [],
   "source": [
    "#pip install pandas numpy scikit-learn lightgbm xgboost catboost imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://claude.ai/chat/7b9ea292-b651-43fe-a26c-ad8e338936fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gb8gM5p72pf0",
    "outputId": "14ce2a42-30e2-458c-a44f-a34614334bff"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# from catboost import CatBoostClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# class CreditRiskModel:\n",
    "#     def __init__(self):\n",
    "#         self.label_encoders = {}\n",
    "#         self.scaler = StandardScaler()\n",
    "#         self.feature_selector = LGBMClassifier()\n",
    "#         self.base_models = {\n",
    "#             'lgbm': LGBMClassifier(random_state=42),\n",
    "#             'xgb': XGBClassifier(random_state=42),\n",
    "#             'catboost': CatBoostClassifier(random_state=42, verbose=False),\n",
    "#             'knn': KNeighborsClassifier(),\n",
    "#             'svm': SVC(probability=True, random_state=42)\n",
    "#         }\n",
    "#         self.meta_learner = XGBClassifier(random_state=42)\n",
    "\n",
    "#     def preprocess_data(self, df):\n",
    "#         # Handle missing values\n",
    "#         numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "#         categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "#         # Fill numeric missing values with median\n",
    "#         for col in numeric_columns:\n",
    "#             df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "#         # Fill categorical missing values with mode\n",
    "#         for col in categorical_columns:\n",
    "#             df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "#         # Encode categorical variables\n",
    "#         for col in categorical_columns:\n",
    "#             self.label_encoders[col] = LabelEncoder()\n",
    "#             df[col] = self.label_encoders[col].fit_transform(df[col])\n",
    "\n",
    "#         return df\n",
    "\n",
    "#     def feature_selection(self, X, y, n_features=50):\n",
    "#         # Train LightGBM for feature importance\n",
    "#         self.feature_selector.fit(X, y)\n",
    "\n",
    "#         # Get feature importance\n",
    "#         importance = pd.DataFrame({\n",
    "#             'feature': X.columns,\n",
    "#             'importance': self.feature_selector.feature_importances_\n",
    "#         })\n",
    "#         importance = importance.sort_values('importance', ascending=False)\n",
    "\n",
    "#         # Select top features\n",
    "#         selected_features = importance['feature'].head(n_features).tolist()\n",
    "#         return X[selected_features]\n",
    "\n",
    "#     def handle_imbalance(self, X, y):\n",
    "#         # Apply SMOTE\n",
    "#         smote = SMOTE(random_state=42)\n",
    "#         X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "#         return X_balanced, y_balanced\n",
    "\n",
    "#     def train_stacking_model(self, X, y):\n",
    "#         # Split data for stacking\n",
    "#         X_train, X_val, y_train, y_val = train_test_split(\n",
    "#             X, y, test_size=0.2, random_state=42\n",
    "#         )\n",
    "\n",
    "#         # Train base models\n",
    "#         meta_features = np.zeros((X_val.shape[0], len(self.base_models)))\n",
    "\n",
    "#         for i, (name, model) in enumerate(self.base_models.items()):\n",
    "#             # Train base model\n",
    "#             model.fit(X_train, y_train)\n",
    "\n",
    "#             # Generate meta-features\n",
    "#             if hasattr(model, 'predict_proba'):\n",
    "#                 meta_features[:, i] = model.predict_proba(X_val)[:, 1]\n",
    "#             else:\n",
    "#                 meta_features[:, i] = model.predict(X_val)\n",
    "\n",
    "#         # Train meta-learner\n",
    "#         self.meta_learner.fit(meta_features, y_val)\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         # Generate meta-features for prediction\n",
    "#         meta_features = np.zeros((X.shape[0], len(self.base_models)))\n",
    "\n",
    "#         for i, (name, model) in enumerate(self.base_models.items()):\n",
    "#             if hasattr(model, 'predict_proba'):\n",
    "#                 meta_features[:, i] = model.predict_proba(X)[:, 1]\n",
    "#             else:\n",
    "#                 meta_features[:, i] = model.predict(X)\n",
    "\n",
    "#         return self.meta_learner.predict(meta_features)\n",
    "\n",
    "#     def evaluate(self, X, y):\n",
    "#         y_pred = self.predict(X)\n",
    "#         return {\n",
    "#             'accuracy': accuracy_score(y, y_pred),\n",
    "#             'precision': precision_score(y, y_pred),\n",
    "#             'recall': recall_score(y, y_pred),\n",
    "#             'f1_score': f1_score(y, y_pred),\n",
    "#             'auc_roc': roc_auc_score(y, y_pred)\n",
    "#         }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zzqFPOHr4OrA",
    "outputId": "e8854d33-5e18-4672-8852-54ecdc8dc91d"
   },
   "outputs": [],
   "source": [
    "accepted_2007_path = fr\"C:\\Users\\krits\\Desktop\\Project\\AI\\TRIS\\CRM dataset\\dataset2\\model2\\data\\accepted_2007_to_2018Q4.csv\"\n",
    "rejected_2007_path = fr\"C:\\Users\\krits\\Desktop\\Project\\AI\\TRIS\\CRM dataset\\dataset2\\model2\\data\\rejected_2007_to_2018Q4.csv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8g8CMCdfpfhs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_accepted_origin = pd.read_csv(accepted_2007_path, low_memory=False)\n",
    "df_rejected_origin = pd.read_csv(rejected_2007_path, low_memory=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accepted = df_accepted_origin.sample(frac=0.03).copy()\n",
    "df_rejected = df_rejected_origin.sample(frac=0.03).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67821"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_accepted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "829462"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_rejected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "r_1dmE0jrLDr"
   },
   "outputs": [],
   "source": [
    "# Create target variable (1 for accepted, 0 for rejected)\n",
    "df_accepted['loan_status_binary'] = (df_accepted['loan_status'] == 'Fully Paid').astype(int)\n",
    "# Store target variable before preprocessing\n",
    "y = df_accepted['loan_status_binary']\n",
    "\n",
    "# Remove target and status columns for features\n",
    "X = df_accepted.drop(['loan_status', 'loan_status_binary'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improve Class CreditRiskModel : preprocess_data\n",
    "https://claude.ai/chat/e68bc3d7-6e72-4fc4-80fc-2849a6b432f0\n",
    "+ Data Scientist : N'Pepsi improve Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreditRiskModel:\n",
    "    def __init__(self):\n",
    "        self.label_encoders = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_selector = LGBMClassifier()\n",
    "        self.base_models = {\n",
    "            'lgbm': LGBMClassifier(random_state=42),\n",
    "            'xgb': XGBClassifier(random_state=42),\n",
    "            'catboost': CatBoostClassifier(random_state=42, verbose=False),\n",
    "            'knn': KNeighborsClassifier(),\n",
    "            'svm': SVC(probability=True, random_state=42)\n",
    "        }\n",
    "        self.meta_learner = XGBClassifier(random_state=42)\n",
    "\n",
    "    def preprocess_data(self, df):\n",
    "        df = df.copy()  # Create a copy to avoid modifying the original dataframe\n",
    "        \n",
    "        # Identify column types\n",
    "        numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "        categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "        \n",
    "        # Handle missing values in numeric columns\n",
    "        for col in numeric_columns:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "        \n",
    "        # Handle missing values and mixed types in categorical columns\n",
    "        for col in categorical_columns:\n",
    "            # Convert all values to string type\n",
    "            df[col] = df[col].astype(str)\n",
    "            \n",
    "            # Replace 'nan' strings with most frequent value\n",
    "            mode_value = df[col].mode()[0]\n",
    "            df[col] = df[col].replace('nan', mode_value)\n",
    "            \n",
    "            # Encode categorical variables\n",
    "            self.label_encoders[col] = LabelEncoder()\n",
    "            df[col] = self.label_encoders[col].fit_transform(df[col])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def feature_selection(self, X, y, n_features=50):\n",
    "        # Train LightGBM for feature importance\n",
    "        self.feature_selector.fit(X, y)\n",
    "\n",
    "        # Get feature importance\n",
    "        importance = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': self.feature_selector.feature_importances_\n",
    "        })\n",
    "        importance = importance.sort_values('importance', ascending=False)\n",
    "\n",
    "        # Select top features\n",
    "        selected_features = importance['feature'].head(n_features).tolist()\n",
    "        return X[selected_features]\n",
    "\n",
    "    def handle_imbalance(self, X, y):\n",
    "        # Apply SMOTE\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "        return X_balanced, y_balanced\n",
    "\n",
    "    def train_stacking_model(self, X, y):\n",
    "        # Split data for stacking\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        # Train base models\n",
    "        meta_features = np.zeros((X_val.shape[0], len(self.base_models)))\n",
    "       \n",
    "        for i, (name, model) in tqdm(enumerate(self.base_models.items()), total=len(self.base_models.items())):\n",
    "            # Train base model\n",
    "            print(\"Model name: \",name)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Generate meta-features\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                meta_features[:, i] = model.predict_proba(X_val)[:, 1]\n",
    "            else:\n",
    "                meta_features[:, i] = model.predict(X_val)\n",
    "\n",
    "        # Train meta-learner\n",
    "        self.meta_learner.fit(meta_features, y_val)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Generate meta-features for prediction\n",
    "        meta_features = np.zeros((X.shape[0], len(self.base_models)))\n",
    "\n",
    "        for i, (name, model) in enumerate(self.base_models.items()):\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                meta_features[:, i] = model.predict_proba(X)[:, 1]\n",
    "            else:\n",
    "                meta_features[:, i] = model.predict(X)\n",
    "        \n",
    "        return self.meta_learner.predict(meta_features)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return {\n",
    "            'accuracy': accuracy_score(y, y_pred),\n",
    "            'precision': precision_score(y, y_pred),\n",
    "            'recall': recall_score(y, y_pred),\n",
    "            'f1_score': f1_score(y, y_pred),\n",
    "            'auc_roc': roc_auc_score(y, y_pred)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CreditRiskModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krits\\anaconda3\\envs\\crm\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "df_processed = model.preprocess_data(df_accepted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and target\n",
    "X = df_processed.drop(['loan_status', 'loan_status_binary'], axis=1)\n",
    "y = df_processed['loan_status_binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 32402, number of negative: 35419\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007301 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 16985\n",
      "[LightGBM] [Info] Number of data points in the train set: 67821, number of used features: 145\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.477758 -> initscore=-0.089028\n",
      "[LightGBM] [Info] Start training from score -0.089028\n"
     ]
    }
   ],
   "source": [
    "# Feature selection\n",
    "X_selected = model.feature_selection(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krits\\anaconda3\\envs\\crm\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "c:\\Users\\krits\\anaconda3\\envs\\crm\\lib\\site-packages\\sklearn\\utils\\_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Handle imbalanced data\n",
    "X_balanced, y_balanced = model.handle_imbalance(X_selected, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_balanced, y_balanced, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name:  lgbm\n",
      "[LightGBM] [Info] Number of positive: 22752, number of negative: 22584\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004081 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10631\n",
      "[LightGBM] [Info] Number of data points in the train set: 45336, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501853 -> initscore=0.007411\n",
      "[LightGBM] [Info] Start training from score 0.007411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:00<00:01,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name:  xgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:00<00:01,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name:  catboost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:08<00:07,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name:  knn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:09<00:02,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name:  svm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [06:05<00:00, 73.18s/it] \n"
     ]
    }
   ],
   "source": [
    "# Train stacking model\n",
    "model.train_stacking_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance:\n",
      "accuracy: 0.9989\n",
      "precision: 0.9983\n",
      "recall: 0.9996\n",
      "f1_score: 0.9989\n",
      "auc_roc: 0.9989\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "results = model.evaluate(X_test, y_test)\n",
    "print(\"Model Performance:\")\n",
    "for metric, value in results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14168, 5)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((X_test.shape[0], 5)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14168, 50)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lgbm': LGBMClassifier(random_state=42),\n",
       " 'xgb': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "               colsample_bylevel=None, colsample_bynode=None,\n",
       "               colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "               enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "               gamma=None, grow_policy=None, importance_type=None,\n",
       "               interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "               max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "               max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "               min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "               multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "               num_parallel_tree=None, random_state=42, ...),\n",
       " 'catboost': <catboost.core.CatBoostClassifier at 0x1f4a202ed90>,\n",
       " 'knn': KNeighborsClassifier(),\n",
       " 'svm': SVC(probability=True, random_state=42)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [0 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0]\n",
      "2 [0 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0]\n",
      "3 [0 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0]\n",
      "4 [0 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0]\n",
      "5 [0 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0]\n",
      "6 [0 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"1\",model.base_models['lgbm'].predict(X_test[20:40]))\n",
    "print(\"2\",model.base_models['xgb'].predict(X_test[20:40]))\n",
    "print(\"3\",model.base_models['catboost'].predict(X_test[20:40]))\n",
    "print(\"4\",model.base_models['knn'].predict(X_test[20:40]))\n",
    "print(\"5\",model.base_models['svm'].predict(X_test[20:40]))\n",
    "print(\"6\",model.predict(X_test[20:40]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "การที่ SVM ใช้เวลาในการ fit นานมากนั้นมีสาเหตุหลักมาจากลักษณะการทำงานของอัลกอริทึม SVM และขนาดของข้อมูลที่คุณกำลังใช้ ในกรณีนี้คุณมี ข้อมูลที่ใหญ่มาก (df_accepted ~200,000 แถว และ df_rejected ~2,000,000 แถว) และเมื่อผ่านขั้นตอน SMOTE ไปแล้ว อาจได้ข้อมูลที่เพิ่มขึ้นอีกหลายเท่าตัว ทำให้จำนวนแถวสูงมากขึ้นไปอีก\n",
    "\n",
    "สาเหตุที่ SVM ใช้เวลานาน:\n",
    "\n",
    "Complexity ของ SVM (โดยเฉพาะ Kernel-based SVM):\n",
    "SVM ที่ใช้ Kernel (เช่น RBF kernel ซึ่งเป็นค่า default ของ sklearn.svm.SVC) จะมี complexity สูงประมาณ O(N²) ถึง O(N³) ในการ train เนื่องจากมันต้องคำนวณ kernel function ระหว่างทุกคู่ข้อมูล ในกรณีที่ N (จำนวนตัวอย่าง) มีค่าหลายแสนหรือหลักล้าน การคำนวณนี้จะใช้เวลานานมาก\n",
    "\n",
    "ขนาดข้อมูลที่ใหญ่: การ training SVM กับข้อมูลระดับ 200,000 - 2,000,000 ตัวอย่างเป็นงานที่ใหญ่มากอยู่แล้ว ในเชิงปฏิบัติ SVM ไม่ค่อยถูกใช้กับข้อมูลที่ใหญ่มากขนาดนี้เพราะใช้ทรัพยากรเวลาและหน่วยความจำมหาศาล\n",
    "\n",
    "ขั้นตอน SMOTE ขยายขนาดข้อมูลเพิ่มขึ้น: SMOTE จะทำการ oversampling class ที่เป็น minority ส่งผลให้จำนวนแถวมากขึ้นจากเดิมที่มีอยู่แล้วมาก ทำให้การ train SVM นั้นยิ่งใช้เวลามากขึ้นอีก\n",
    "\n",
    "การใช้ Kernel ที่ไม่เหมาะสมกับข้อมูลขนาดใหญ่: RBF kernel มักจะให้ผลลัพธ์ดี แต่ต้องแลกมาด้วยเวลา train ที่สูง หากจำเป็นต้องใช้ SVM จริงๆ กับข้อมูลขนาดใหญ่มาก อาจต้องพิจารณาการใช้ Linear SVM (LinearSVC) หรือใช้ Approximate Methods อื่น ๆ\n",
    "\n",
    "วิธีแก้ไขหรือบรรเทาปัญหา:\n",
    "\n",
    "ลดขนาดข้อมูล (Dimensionality & Sample Size):\n",
    "\n",
    "อาจลดจำนวนคุณลักษณะลง หรือใช้วิธี feature selection เข้มข้นขึ้น รวมไปถึงการทำ PCA หรือ Random Projection เพื่อลดมิติลง\n",
    "ลดจำนวนตัวอย่างลง โดยสุ่ม sample ข้อมูลบางส่วนมา train แทนทั้งหมด (ถ้าจำเป็น)\n",
    "ใช้ Linear SVC หรือ Kernel ที่ง่ายกว่า:\n",
    "\n",
    "ลองใช้ LinearSVC ซึ่งมี complexity O(N) หรือ O(N x features) โดยไม่คำนวณ pairwise kernel ระหว่างทุกตัวอย่าง จะเร็วกว่ามากถ้าข้อมูลมีจำนวนแถวเยอะ\n",
    "หากจำเป็นต้องใช้ kernel อาจลองใช้ Approximation Techniques เช่น Random Fourier Features หรือ Nystroem method เพื่อลด complexity\n",
    "เปลี่ยนไปใช้โมเดลอื่นที่สเกลได้ดีกับข้อมูลขนาดใหญ่:\n",
    "\n",
    "โมเดลที่ใช้ Gradient Boosting อย่าง LightGBM, XGBoost หรือ CatBoost มักสเกลได้ดีกว่า SVM เมื่อจำนวนแถวข้อมูลใหญ่\n",
    "Neural Networks หรือ Logistic Regression ก็อาจ train ได้เร็วกว่าในบางกรณี\n",
    "โดยสรุป สาเหตุหลักที่ SVM ใช้เวลา fit นานมาก เพราะ SVM มีความซับซ้อนเชิงเวลาสูงเมื่อข้อมูลมีขนาดใหญ่มาก โดยเฉพาะเมื่อใช้ kernel trick กับจำนวนข้อมูลระดับหลายแสนถึงหลักล้าน อีกทั้งการใช้ SMOTE ยังเพิ่มขนาดข้อมูลเข้าไปอีก ทำให้การคำนวณยิ่งหนักขึ้น จึงควรพิจารณาเปลี่ยนวิธีหรือลดขนาดปริมาณงานที่โมเดลต้องทำลง"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "crm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
